# Отчет к ДЗ
Выполнила: Усова Валентина, ИСП-221С
## Задача
Построить модель логистической регрессии для классификации двух классов на основе предоставленных данных. Оценить качество модели и сравнить удобство работы в Google Colab и Kaggle.

## 1. Подготовка данных

Начинаем с импорта необходимых библиотек:
```
import numpy as np  # Импортируем библиотеку NumPy для работы с массивами
import matplotlib.pyplot as plt  # Импортируем библиотеку Matplotlib для построения графиков
from sklearn.model_selection import train_test_split  # Импортируем функцию для разделения данных на тренировочный и тестовый наборы
from sklearn.metrics import accuracy_score, classification_report  # Импортируем функции для оценки модели
```
Затем определяем входные данные: (были взяты из примера)
```
x1 = [0.38, 1.42, 0.55, 1.34, 1.76, 1.62, 0.83, 0.84, 1.77, 1.06]
y1 = [1.79, 0.54, 0.34, 0.678, 1.64, 0.92, 1.49, 0.3, 0.7, 0.99]
x2 = [3.9, 6.14, 6.1, 2.11, 3.23, 1.62, 1.88]
y2 = [4.93, 4.95, 0.97, 0.77, 0.43, 4.61, 0.25]
```
Данные были представлены в виде двух наборов координат (x, y) для двух классов. Эти координаты были объединены в матрицу признаков features, где каждый ряд представляет точку данных, а столбцы соответствуют координатам x и y. Соответствующий вектор меток labels был создан, присваивая 0 первому классу и 1 второму классу. Этот формат данных (матрица признаков и вектор меток) был использован для обучения модели логистической регрессии.
```
features1 = np.column_stack((x1, y1))
features2 = np.column_stack((x2, y2))
features = np.concatenate((features1, features2))
labels = np.concatenate((np.zeros(len(x1)), np.ones(len(x2))))
```
Выполняем стандартизацию признаков, выражающаюся в вычитании среднего значения каждого признака и делении на его стандартное отклонение. Это позволит привести данные к нулевому среднему и единичной дисперсии, что улучшает производительность многих алгоритмов машинного обучения, включая логистическую регрессию.
```
features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)
```
Разделяем данные на тренировочный и тестовый наборы. Функция train_test_split из библиотеки scikit-learn использовалась для разделения данных в соотношении 80% для обучения модели (тренировочный набор) и 20% для её оценки (тестовый набор). Параметр random_state=42 гарантирует воспроизводимость результатов при многократном запуске кода.
```
features_train, features_test, labels_train, labels_test = train_test_split(
    features, labels, test_size=0.2, random_state=42
) 
```
## 2. Модель и Обучение
Определяем гиперпараметры: скорость обучения (learning_rate = 0.01), количество эпох (epochs = 1500) и коэффициент L2-регуляризации (lambda_reg = 0.1). L2-регуляризация используется для предотвращения переобучения модели, штрафуя большие значения весов.
```
learning_rate = 0.01
epochs = 1500
lambda_reg = 0.1  # Коэффициент L2-регуляризации
```
Реализуем необходимые функции: сигмоиду, предсказание и функцию потерь
```
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def predict(X, weights):
    z = np.dot(X, weights[:-1]) + weights[-1]
    return sigmoid(z)

def loss(y_true, y_pred, weights, lambda_reg):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    regularization_term = (lambda_reg / 2) * np.sum(weights[:-1]**2)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) + regularization_term

```
Инициализируем веса и запускаем градиентный спуск. Модель обучалась, постепенно подстраивая свои внутренние параметры (веса) так, чтобы уменьшить ошибку предсказаний. Этот процесс повторялся 1500 раз, и его ход отслеживался по уменьшению ошибки.
```
weights = np.random.randn(features.shape[1] + 1)

for epoch in range(epochs):
    y_pred = predict(features_train, weights)
    error = y_pred - labels_train
    gradient = np.dot(features_train.T, error) / len(features_train)
    gradient = np.concatenate(([np.mean(error)], gradient))

    weights[:-1] -= learning_rate * (gradient[1:] + lambda_reg * weights[:-1])
    weights[-1] -= learning_rate * gradient[0]

    if (epoch + 1) % 100 == 0:
        print(f"Эпоха {epoch+1}/{epochs}, Потери: {loss(labels_train, y_pred, weights, lambda_reg)}") Объясни для отчета
```
## 3. Оценка Модели
После обучения, модель проверили на новых данных (тестовый набор), которые она раньше не видела. Модель предсказала класс для каждой точки в тестовом наборе (y_pred_test). Вероятности, выданные моделью, были переведены в классы (0 или 1) — если вероятность больше 0.5, точка относилась к классу 1, иначе к классу 0 (y_pred_test_classes).

Затем посчитали, какой процент предсказаний был правильным — это и есть точность (accuracy). Кроме точности, был получен более подробный отчет (classification_report), который показал, насколько хорошо модель предсказывает каждый класс отдельно (precision, recall, F1-score). Этот отчет дает более полное представление о качестве модели, чем только точность.
```
y_pred_test = predict(features_test, weights)
y_pred_test_classes = (y_pred_test > 0.5).astype(int)
accuracy = accuracy_score(labels_test, y_pred_test_classes)
print(f"\nТочность на тестовом наборе: {accuracy:.4f}")
print("Отчет по классификации:")
print(classification_report(labels_test, y_pred_test_classes, zero_division=0)) расскажи по простому подробнее
```
## 4. Визуализация
Для наглядности результатов обучения была построена диаграмма рассеяния. На ней изображены точки данных, разделенные по классам (кружки для класса 0, крестики для класса 1). Также на графике показана прямая линия — это разделяющая гиперплоскость, найденная моделью логистической регрессии. Эта линия разделяет пространство признаков таким образом, что точки по разные стороны от нее относятся к разным классам. Визуализация позволяет оценить качество работы модели: чем лучше разделение точек, тем лучше модель.
```
    plt.figure(figsize=(8, 6))
    plt.scatter(features[labels == 0, 0], features[labels == 0, 1], label='Класс 0', marker='o')  # Точки первого класса
    plt.scatter(features[labels == 1, 0], features[labels == 1, 1], label='Класс 1', marker='x')  # Точки второго класса

    xx = np.linspace(np.min(features[:, 0]), np.max(features[:, 0]), 100)  # Диапазон значений для построения линии
    yy = -(weights[0] * xx + weights[2]) / weights[1]  # Уравнение разделяющей линии
    plt.plot(xx, yy, 'r-', label="Разделяющая линия")  # Построение разделяющей линии

    plt.xlabel('Признак 1')
    plt.ylabel('Признак 2')
    plt.title('Логистическая регрессия')
    plt.legend()
    plt.grid(True)
    plt.show()
```
![image](https://github.com/user-attachments/assets/a3a55308-3c2d-4efc-b232-c4f8a000e771)

## 5. Сравнение работы на Google Colab и Kaggle
В данном проекте для проведения экспериментов по машинному обучению были использованы две популярные облачные платформы: Google Colab и Kaggle.

## Google Colab
![image](https://github.com/user-attachments/assets/2bc4c955-c72c-41ae-b335-2d6bf0622970)

Преимущества:

• Бесплатный доступ к ресурсам: Colab предоставляет бесплатный доступ к вычислительным мощностям, включая GPU, что значительно ускоряет процесс обучения моделей, особенно для ресурсоемких задач.

• Простота использования: Интуитивно понятный интерфейс, основанный на Jupyter Notebook, упрощает начало работы и позволяет быстро приступить к экспериментированию. Минимальная настройка среды разработки экономит время.

Недостатки:

• Ограничения по времени сессии: Длительное бездействие может привести к прерыванию сессии и потере несохраненных данных.

• Ограниченный доступ к файловой системе: Работа с большими наборами данных может быть затруднена из-за ограничений на доступ к локальным файлам и необходимость интеграции с внешними хранилищами.

## Kaggle
![image](https://github.com/user-attachments/assets/23c7bb1c-8bcb-45fc-9fc8-8c7c0622f477)

Преимущества:

• Мощные вычислительные ресурсы: Kaggle предоставляет доступ к высокопроизводительным GPU и TPU, что делает его идеальным вариантом для работы со сложными моделями и масштабными наборами данных.

• Обширная библиотека датасетов: Удобный доступ к огромному количеству общедоступных наборов данных, упрощающий поиск и загрузку информации для экспериментов.

• Инструменты для совместной работы: Встроенные функции для совместной работы облегчают взаимодействие с другими исследователями.

Недостатки:

• Платные ресурсы: Некоторые вычислительные ресурсы и функции доступны только за плату.

• Более сложный интерфейс: Для новичков освоение платформы может потребовать больше времени по сравнению с Colab.


## Выбор платформы для данного проекта
Для нашего проекта Google Colab подошел идеально: он был прост в использовании и предоставил все необходимые ресурсы бесплатно. Использование Kaggle было излишним и усложнило работу.
